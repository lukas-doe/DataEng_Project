# Real-time Streaming Backend mit Apache Flink, Kafka & PostgreSQL

Dieses Projekt entstand im Rahmen des Moduls **Data Engineering (DLMDWWDE02)** an der IU und zeigt eine vollstÃ¤ndige Echtzeit-Streaming-Architektur auf Basis von Apache Flink, Kafka, PostgreSQL und Python.

## Zielsetzung

Ziel des Projekts ist es, ein skalierbares Backend fÃ¼r eine datenintensive Applikation aufzubauen, das Sensordaten in Echtzeit verarbeitet, filtert und in eine relationale Datenbank schreibt.

---

## Systemarchitektur

Die Anwendung besteht aus folgenden Services:

- **Producer (Python)**: Generiert simulierte Sensordaten und sendet sie an Kafka.
- **Kafka (Confluent Platform)**: Message Broker fÃ¼r die Echtzeit-Datenverteilung.
- **Apache Flink (SQL)**: Echtzeit-Streamverarbeitung und Filterung der Daten.
- **PostgreSQL**: Persistenzschicht fÃ¼r gefilterte Sensordaten.
- **Flask Dashboard (optional)**: Visualisiert Temperaturen >â€¯30â€¯Â°C in einem Webinterface.

---

## Komponenten

| Service            | Aufgabe                            |
|--------------------|-------------------------------------|
| `producer.py`      | Simuliert Temperaturdaten           |
| `flink-job`        | Flink SQL Setup + Kafka/JDBC JARs   |
| `job.sql`          | Daraus werden Source/Sink-Tabellen + Filter-Logik erstellt|
| `postgres-init`    | Initialisiert PostgreSQL-Tabelle    |
| `docker-compose`   | Orchestriert das gesamte Setup      |

---

## Setup

### ğŸ”§ Voraussetzungen

- Docker + Docker Compose
- (Optional) Python & Flask fÃ¼r lokale Dashboard-Nutzung

### Start

```bash
docker compose up --build
```

**Hinweis**: Der Flink SQL-Job (`INSERT INTO ...`) muss derzeit **manuell** im SQL-Client ausgefÃ¼hrt werden. Siehe unten.

---

## Hintergrund zur manuellen AusfÃ¼hrung

**Warum wird der INSERT nicht automatisch ausgefÃ¼hrt?**

Aufgrund des Timings zwischen dem Flink-SQL-Client und der VerfÃ¼gbarkeit des PostgreSQL-Containers lieÃŸ sich das `INSERT INTO`-Statement im automatisierten Docker-Setup nicht zuverlÃ¤ssig ausfÃ¼hren.

**Workaround**: Der SQL-Job wird manuell Ã¼ber den Flink SQL-Client gestartet.  
Alle anderen Komponenten (Kafka, Producer, PostgreSQL, Flink) laufen automatisch im Compose-Setup.

---

## Nachdem Docker Compose gestartet wurde, muss Flink Job wie folgt im Terminal gestartet werden:

### 1. SQL-Client manuell Ã¶ffnen, Flink Jobmanager Container ID kann Ã¼ber Befehl "docker ps" im Terminal ermittelt werden:

```bash
docker exec -it <flink-jobmanager-container-id> /bin/bash
sql-client.sh --embedded
```

### 2. Statements aus `job.sql` manuell eingeben, darauf achten dass immer nur ein Statement und danach das jeweils nÃ¤chste geschrieben wird:

```sql
-- Tabellen & Inserts siehe: flink-job/job.sql
```

---

## Visualisierung

Starte das Flask-Dashboard lokal:

```bash
cd dashboard
pip install -r requirements.txt
python app.py
```

â¡ï¸ Ã–ffne `http://localhost:5000`  
Das Dashboard zeigt alle Temperaturen Ã¼ber 30â€¯Â°C an.

---

## Verzeichnisstruktur

```
.
â”œâ”€â”€ producer/                 # Python-Kafka Producer
â”œâ”€â”€ flink-job/               # Flink-SQL + JARs
â”‚   â”œâ”€â”€ job.sql
â”‚   â””â”€â”€ lib/
â”œâ”€â”€ postgres-init/           # SQL Init-Skript fÃ¼r PostgreSQL
â”œâ”€â”€ dashboard/               # Flask-Webapp (optional)
â”œâ”€â”€ docker-compose.yml
```

---

## Lizenz

Dieses Projekt wurde im Rahmen des IU-Moduls *Data Engineering* entwickelt und dient ausschlieÃŸlich Bildungszwecken.

---

## Autor

**Lukas DÃ¶bbelin**  
Studium Wirtschaftsingenieurwesen (M.Eng.), Schwerpunktmodule: AI & Data Engineering